{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe382477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = SECRET_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cc8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"What is {question}?\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "try:\n",
    "    print(chain.invoke({\"question\": \"quantum computing\"}))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68107e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\", temperature=0.7)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"What is {question}?\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "try:\n",
    "    print(chain.invoke({\"question\": \"quantum computing\"}))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\", temperature=0.0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Suggest a name for a company that makes {product}?\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "try:\n",
    "    print(chain.invoke({\"product\": \"american food\"}))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that can translate {input_language} to {output_language}.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "print(prompt_template.format(\n",
    "    input_language=\"English\",\n",
    "    output_language=\"Bangla\",\n",
    "    text=\"Hello, how are you?\"\n",
    "))\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\", temperature=0.0)\n",
    "\n",
    "chain = prompt_template | model | StrOutputParser()\n",
    "\n",
    "try:\n",
    "    print(chain.invoke({\"input_language\": \"English\", \"output_language\": \"Bangla\", \"text\": \"Hello, how are you?\"}))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chain.stream({\n",
    "    \"input_language\": \"English\",\n",
    "    \"output_language\": \"Bangla\",\n",
    "    \"text\": \"Hello, how are you?\"\n",
    "}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chain.batch([\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"Bangla\",\n",
    "        \"text\": \"Hello, how are you?\"\n",
    "    },\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"Bangla\",\n",
    "        \"text\": \"What is your name?\"\n",
    "    }\n",
    "])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=[\"product\"], \n",
    "    template=\"Suggest a name for a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "print(prompt_template_name.format(product=\"american food\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9718d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template_name | model | StrOutputParser()\n",
    "\n",
    "try:\n",
    "    print(chain.invoke({\"product\": \"american food\"}))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13947bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.0)\n",
    "\n",
    "itinerary_template = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "    \"system\", \n",
    "    \"\"\"\n",
    "    You are an expert travel scout. \n",
    "    Your job is to create a 2-day itinerary for the user. \n",
    "    Be realistic about travel times and stick strictly to the user's budget level.\n",
    "    \"\"\"\n",
    "     ),\n",
    "    (\"human\", \"I am going to {destination} and my budget is {budget_level}. I am interested in {interests}.\")\n",
    "])\n",
    "\n",
    "travel_chain = itinerary_template | llm | StrOutputParser()\n",
    "\n",
    "def generate_trip():\n",
    "    destination = input(\"Enter your travel destination: \")\n",
    "    budget_level = input(\"Enter your budget level (low, medium, high): \")\n",
    "    interests = input(\"Enter your interests (comma-separated): \")\n",
    "\n",
    "    for chunk in travel_chain.stream({\n",
    "        \"destination\": destination,\n",
    "        \"budget_level\": budget_level,\n",
    "        \"interests\": interests\n",
    "    }):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "generate_trip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrival Augmented Generation (RAG)\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load and split documents\n",
    "loader = PDFPlumberLoader(\"01. Alice's Adventures in Wonderland author Lewis Caroll.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e047d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in Vector database\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_documents(documents) -> str:\n",
    "    return \"\\n\\n\".join([document.page_content for document in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a55643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RAG chain\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.0)\n",
    "prompt = ChatPromptTemplate.from_template(\"Answer based ONLY on context: {context}\\nQuestion: {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d32a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(lambda x: x[\"question\"]) | retriever | RunnableLambda(format_documents), \n",
    "        \"question\": RunnablePassthrough(), \n",
    "        \"history\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\n",
    "    rag_chain.invoke(\n",
    "        {\"question\": \"Who is the author of Alice's Adventures in Wonderland?\"}\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational Memory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    rag_chain, \n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": \"1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "while True:\n",
    "    _input = input(\"Enter a question (or 'exit' to quit): \")\n",
    "    if _input == \"exit\":\n",
    "        break\n",
    "    response = with_message_history.invoke(\n",
    "        {\"question\": _input},\n",
    "        config=config\n",
    "    )\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df1ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Setup local llm and embeddings\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.7)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load and split documents\n",
    "loader = PDFPlumberLoader(\"01. Alice's Adventures in Wonderland author Lewis Caroll.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Load the library from the persistent directory\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Conversational prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that answers questions based on the provided {context}.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"Question: {question}\")\n",
    "])\n",
    "\n",
    "def format_documents(documents) -> str:\n",
    "    return \"\\n\\n\".join([document.page_content for document in documents])\n",
    "\n",
    "\n",
    "# Build the RAG chain with conversational memory\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: format_documents(retriever.invoke(x[\"question\"])), \n",
    "        \"question\": lambda x: x[\"question\"], \n",
    "        \"history\": lambda x: x[\"history\"]\n",
    "    } \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# Wrap with conversational memory\n",
    "chain = RunnableWithMessageHistory(\n",
    "    rag_chain, \n",
    "    get_session, \n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": \"1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\n",
    "    chain.invoke(\n",
    "        {\n",
    "            \"question\": \"Who is the author of Alice's Adventures in Wonderland?\"\n",
    "        }, \n",
    "        config=config\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that can translate {text} from {input_language} to {output_language}.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "print(prompt.format(\n",
    "    input_language = \"English\",\n",
    "    output_language = \"Bangla\",\n",
    "    text = \"Hello, how are you?\"\n",
    "))\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\", temperature=0.7)\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "try:\n",
    "    print(chain.invoke({\"input_language\": \"English\", \"output_language\": \"Bangla\", \"text\": \"Hello, how are you?\"}))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414cc9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that can suggest some {product} company names. Please provide the names in a comma-separated format.\"),\n",
    "    (\"human\", \"Suggest some company names for a company that makes {product}.\")\n",
    "])\n",
    "\n",
    "chain = prompt | model | CommaSeparatedListOutputParser()\n",
    "\n",
    "try:\n",
    "    print(chain.invoke({\"product\": \"american food\"}))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f71a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CompanyNames(BaseModel):\n",
    "    names: list[str] = Field(..., description=\"A list of company names\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that can suggest some {product} company names. Please provide the names in {format_instruction}.\"),\n",
    "    (\"human\", \"Suggest some company names for a company that makes {product}.\")\n",
    "])\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=CompanyNames)\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "try:\n",
    "    print(chain.invoke({\n",
    "        \"product\": \"american food\", \n",
    "        \"format_instruction\": parser.get_format_instructions()\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0165ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "document_a = Document(page_content=\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"What is the summary of the following document? {document}\")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "try:\n",
    "    print(chain.invoke({\"document\": document_a}))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.langchain.com/\")\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "splits = splitter.split_documents(documents)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "def format_documents(documents) -> str:\n",
    "    return \"\\n\\n\".join([document.page_content for document in documents])\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\", temperature=0.0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Answer based ONLY on context: {context}\\nQuestion: {question}\")\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: format_documents(retriever.invoke(x[\"question\"])), \n",
    "        \"question\": lambda x: x[\"question\"]\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = rag_chain.invoke(\n",
    "    {\"question\": \"What is LangChain?\"}\n",
    "    )\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36c5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
